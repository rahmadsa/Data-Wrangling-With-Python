{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kkSXsFae8Cct"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32fjpkeS-nYP"
      },
      "source": [
        "#Getting Started with PySpark in Google Colab\n",
        "\n",
        "PySpark adalah interface Python untuk Apache Spark. Penggunaan utama PySpark adalah untuk bekerja dengan data dalam Bigdata dan untuk membuat pipeline data.\n",
        "\n",
        "Walalupun Apache Spark mendudukung Big Data, sebagai awal pembelajaran tidak perlu menggunakan data yang besar untuk mendapatkan manfaat dari PySpark. Kita bisa temukan bahwa SparkSQL adalah tools yang bagus untuk melakukan analisis data. Penggunaan library Panda menjadi lambat dengan data yang besar\n",
        "\n",
        "Sumber tentang Apache Spark http://spark.apache.org/docs/latest/api/python/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "# 1. Installing PySpark in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxv7w_2y2bb9"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memulai Spark"
      ],
      "metadata": {
        "id": "fH6T0ODj_HT_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcOCBgQo2Pqf"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHvKMqLQ4ezk"
      },
      "source": [
        "# 2. Reading Data\n",
        "\n",
        "Sebagai contoh pembacaan sumber data, Kita akan menggunakan public  data set in dalam CSV format.\n",
        "Sumber data:\n",
        "https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzvNxiQSixRU"
      },
      "source": [
        "import requests\n",
        "path = \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv\"\n",
        "req = requests.get(path)\n",
        "url_content = req.content\n",
        "\n",
        "csv_file_name = 'owid-covid-data.csv'\n",
        "csv_file = open(csv_file_name, 'wb')\n",
        "\n",
        "csv_file.write(url_content)\n",
        "csv_file.close()\n",
        "\n",
        "df = spark.read.csv('/content/'+csv_file_name, header=True, inferSchema=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYRUC46L_8zX"
      },
      "source": [
        "#3. PySpark DataFrames\n",
        "\n",
        "**DataFrame** adalah struktur data dua dimensi dalam bahasa pemrograman komputer, mirip dengan tabel pada microsoft Excel. Pada pemrograman Python strukur data DataFrame adalah objek dalam pustaka panda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-PgzP3IjZsV"
      },
      "source": [
        "#Viewing the dataframe schema\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KKBv0ZCFbP5"
      },
      "source": [
        "#Converting date column\n",
        "df.select(F.to_date(df.date).alias('date'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2ylA4B2kfd2"
      },
      "source": [
        "#Summary stats\n",
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRX6qF_dEp9l"
      },
      "source": [
        "#DataFrame Filtering\n",
        "df.filter(df.location == \"United States\").orderBy(F.desc(\"date\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzGaFJ3QEG19"
      },
      "source": [
        "#Simple Group by Function\n",
        "df.groupBy(\"location\").sum(\"new_cases\").orderBy(F.desc(\"sum(new_cases)\")).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt6e41cFEFSR"
      },
      "source": [
        "# 4. Spark SQL\n",
        "\n",
        "Salah satu kelebihan dari apache Spark adalah modul SQL, modul ini sangat memudahkan untuk berinteraksi dengan data menggunakan Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBpoPIGDrb-c"
      },
      "source": [
        "#Creating a table from the dataframe\n",
        "df.createOrReplaceTempView(\"covid_data\") #temporary view\n",
        "# df.saveAsTable(\"covid_data\") #Save as a table\n",
        "# df.write.mode(\"overwrite\").saveAsTable(\"covid_data\") #Save as table and overwrite table if exits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFcoi5l7kyLq"
      },
      "source": [
        "\n",
        "df2 = spark.sql(\"SELECT * from covid_data\")\n",
        "df2.printSchema()\n",
        "df2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teHD2Up4k4Cd"
      },
      "source": [
        "groupDF = spark.sql(\"SELECT location, count(*) from covid_data group by location\")\n",
        "groupDF.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTvI4jbZjX31"
      },
      "source": [
        "# 5. Tugas Penggunaan Data Set\n",
        "Penggunaan data set Google Colab Session\n",
        "\n",
        "https://github.com/rahmadsa/dataset/blob/main/sample_csv.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-To1oW2S4mZL"
      },
      "source": [
        "df = spark.read.csv(\"/sample_csv.csv\", header=True, inferSchema=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvDrXp8w4pFi"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-ra4P9Z7sut"
      },
      "source": [
        "#print N rows\n",
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKzuJaaw79Kf",
        "outputId": "04a1343c-a572-4a6f-d93f-d09446177494"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17000"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvjbab-J7_t_"
      },
      "source": [
        "df.select(\"order_date\",\"province\").show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAzEcJp78NIH"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl85UrLC8PYW",
        "outputId": "521abebd-2b42-4506-9475-909f9d016517"
      },
      "source": [
        "df.select('item_price').distinct().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|total_rooms|\n",
            "+-----------+\n",
            "|      934.0|\n",
            "|     3980.0|\n",
            "|     4142.0|\n",
            "|      596.0|\n",
            "|     1761.0|\n",
            "|     5983.0|\n",
            "|     2815.0|\n",
            "|     6433.0|\n",
            "|      299.0|\n",
            "|     2734.0|\n",
            "|      769.0|\n",
            "|     1051.0|\n",
            "|     7554.0|\n",
            "|     4066.0|\n",
            "|     2862.0|\n",
            "|     3597.0|\n",
            "|      692.0|\n",
            "|      720.0|\n",
            "|     1765.0|\n",
            "|     2523.0|\n",
            "+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5tOUkPJ8XRb"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "test = df.groupBy('item_price').agg(F.sum('city'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv6f3pGb_XR8"
      },
      "source": [
        "test.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaUs3Q-F8lA3"
      },
      "source": [
        "#Counting and removing missing values\n",
        "\n",
        "df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE4zy4gdbLnE"
      },
      "source": [
        "# Creating a Test Spark DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "750m7yL29U4g"
      },
      "source": [
        "data = [\n",
        "        ('John','Smith',1),\n",
        "        ('Jane','Smith',2),\n",
        "        ('Jonas','Smith',3),\n",
        "]\n",
        "\n",
        "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17sJ9jaliV-b"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP1GBfIoxIKw"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbsONR7IRZnl"
      },
      "source": [
        "# Spark Tips and Tricks\n",
        "\n",
        "Contih code snippets for common or tricky penggunaan apache spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pandas DataFrame to Spark DataFrame"
      ],
      "metadata": {
        "id": "kkSXsFae8Cct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(np.random.randint(100,size=(1000, 3)),columns=['A','B','C'])\n",
        "spark_df = spark.createDataFrame(df)\n",
        "spark_df.show()"
      ],
      "metadata": {
        "id": "858-wUBy8BhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Object columns in pandas dataframe to a string\n",
        "for i in df.select_dtypes(include='object').columns.tolist():\n",
        "\tdf[i] = df[i].astype(str)\n",
        "\n",
        "#Convert datetimes to UTC\n",
        "  for i in [col for col in df.columns if df[col].dtype == 'datetime64[ns]']:\n",
        "   df[i] = pd.to_datetime(df[i], utc=True)\n",
        "\n",
        "#Replace nan and \"None\" in pandas dataframe to null in the spark dataframe\n",
        "spark_df = spark.createDataFrame(df).replace('None', None).replace(float('nan'), None)"
      ],
      "metadata": {
        "id": "M3C2b3PX-7FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGa0hwxCRlEf"
      },
      "source": [
        "##Window Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIL2p4P3Z6bu"
      },
      "source": [
        "data = [\n",
        "        (1,'2021-01-01 10:00:00'),\n",
        "        (1,'2021-01-01 11:00:00'),\n",
        "        (1,'2021-01-01 12:00:00'),\n",
        "        (2,'2021-01-01 12:00:00'),\n",
        "        (2,'2021-01-01 13:00:00'),\n",
        "        (2,'2021-01-01 14:00:00'),\n",
        "]\n",
        "\n",
        "columns = [\"id\",\"datetime\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.createOrReplaceTempView(\"window_test\")\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6ZCcOiCZ74G"
      },
      "source": [
        "#Selecting the min and max by a specific Group\n",
        "spark.sql('''\n",
        "Select\n",
        "  id,\n",
        "\n",
        "  max(datetime) OVER (Partition BY id ORDER BY datetime) as max_date,\n",
        "  min(datetime) OVER (Partition BY id ORDER BY datetime) as min_date,\n",
        "\n",
        "  ROW_NUMBER() OVER (Partition BY id ORDER BY datetime) as row_number\n",
        "\n",
        "  FROM window_test\n",
        "\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouYaOvMDcYYS"
      },
      "source": [
        "# Selecting the row number or order rank for each row within a specified grouping.\n",
        "# This is great for sub rankings in a table\n",
        "\n",
        "spark.sql('''\n",
        "Select\n",
        "  id,\n",
        "  datetime,\n",
        "\n",
        "  ROW_NUMBER() OVER (Partition BY id ORDER BY datetime) as row_number\n",
        "\n",
        "  FROM window_test\n",
        "\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## De-duplicate data dalam sebuah row dataset menggunakan window function"
      ],
      "metadata": {
        "id": "eJgMqCiJGfZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "        (1,'2021-01-01',100,'A'),\n",
        "        (1,'2021-01-31',105,'A'),\n",
        "        (2,'2021-02-04',160,'B'),\n",
        "        (2,'2021-02-07',145,'B'),\n",
        "]\n",
        "\n",
        "columns = [\"id\",\"date\",\"score\",\"type\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.createOrReplaceTempView(\"window_test\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "JBbA3LY3GwEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.sql(\"\"\"\n",
        "WITH T AS (\n",
        "  SELECT\n",
        "  *,\n",
        "  ROW_NUMBER() OVER (PARTITION BY id ORDER BY date DESC) AS version_number\n",
        "  FROM window_test\n",
        ")\n",
        "\n",
        "SELECT * FROM T WHERE version_number = 1;\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "HwhLxFkibR0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "  SELECT\n",
        "  *,\n",
        "  SUM(score) OVER (PARTITION by type ORDER BY date) as score_cumulative\n",
        "  FROM window_test\n",
        "\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TByf27YO6rdt",
        "outputId": "26f90838-3897-4e79-c466-45120441792f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+-----+----+----------------+\n",
            "| id|      date|score|type|score_cumulative|\n",
            "+---+----------+-----+----+----------------+\n",
            "|  1|2021-01-01|  100|   A|             100|\n",
            "|  1|2021-01-31|  105|   A|             205|\n",
            "|  2|2021-02-04|  160|   B|             160|\n",
            "|  2|2021-02-07|  145|   B|             305|\n",
            "+---+----------+-----+----+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limit results per group dengan window function"
      ],
      "metadata": {
        "id": "hDNlXaL0QEPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame(\n",
        "np.hstack((\n",
        "    np.random.randint(1,5,size=(100000, 1)),\n",
        "    np.random.randint(100,size=(100000, 1))\n",
        "))\n",
        ", columns=['company_id', 'number'])\n",
        "\n",
        "dff = spark.createDataFrame(df)\n",
        "dff.createOrReplaceTempView(\"window_test_limits\")\n"
      ],
      "metadata": {
        "id": "pSyMC_ypQIfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "WITH T AS (\n",
        "  SELECT\n",
        "    company_id,\n",
        "    number,\n",
        "    ROW_NUMBER() OVER (PARTITION BY company_id ORDER BY number) AS row_number\n",
        "  FROM window_test_limits\n",
        "    )\n",
        "\n",
        "SELECT * FROM T WHERE row_number <= 100\n",
        "\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLSEEW6VQNLa",
        "outputId": "c8900973-56e0-4503-f031-144ff5331023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+----------+\n",
            "|company_id|number|row_number|\n",
            "+----------+------+----------+\n",
            "|         1|     0|         1|\n",
            "|         1|     0|         2|\n",
            "|         1|     0|         3|\n",
            "|         1|     0|         4|\n",
            "|         1|     0|         5|\n",
            "|         1|     0|         6|\n",
            "|         1|     0|         7|\n",
            "|         1|     0|         8|\n",
            "|         1|     0|         9|\n",
            "|         1|     0|        10|\n",
            "|         1|     0|        11|\n",
            "|         1|     0|        12|\n",
            "|         1|     0|        13|\n",
            "|         1|     0|        14|\n",
            "|         1|     0|        15|\n",
            "|         1|     0|        16|\n",
            "|         1|     0|        17|\n",
            "|         1|     0|        18|\n",
            "|         1|     0|        19|\n",
            "|         1|     0|        20|\n",
            "+----------+------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kalkulasi 7 hari moving average"
      ],
      "metadata": {
        "id": "T0Ynq3qAVWs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(pd.date_range('1/1/2022','1/31/2022',freq='D'), columns=['date'])\n",
        "import random\n",
        "df['company_id'] = 1\n",
        "df['number'] = df.apply(lambda x: random.randint(0,100), axis = 1)\n",
        "\n",
        "dff = spark.createDataFrame(df)\n",
        "dff.createOrReplaceTempView(\"window_data\")\n",
        "\n",
        "dff.show()"
      ],
      "metadata": {
        "id": "e1Rkc0bCVaoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "  date,\n",
        "  company_id,\n",
        "  number,\n",
        "  AVG(number) OVER (PARTITION BY company_id ORDER BY date ASC RANGE BETWEEN INTERVAL 6 DAYS PRECEDING AND CURRENT ROW) as last_7_day_avg\n",
        "FROM window_data\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "atZLJhqIXYIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monthly Active Users"
      ],
      "metadata": {
        "id": "yjKv-xeZKOdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(pd.date_range('1/1/2022','1/31/2022',freq='D'), columns=['login_date'])\n",
        "import random\n",
        "df['company_id'] = 1\n",
        "df['user_id'] = df.apply(lambda x: random.randint(0,3), axis = 1)\n",
        "\n",
        "dff = spark.createDataFrame(df)\n",
        "dff.createOrReplaceTempView(\"users_data\")\n",
        "\n",
        "dff.show()"
      ],
      "metadata": {
        "id": "KP7bOaRnKZO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Revisit this transform\n",
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "  login_date,\n",
        "  COUNT(user_id) OVER (PARTITION BY login_date ORDER BY login_date ASC RANGE BETWEEN INTERVAL 30 DAYS PRECEDING AND CURRENT ROW) AS monthly_active_users\n",
        "  FROM users_data\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "rtaaCPuZKfTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mencari time difference antara dataset rows menggunakan a window function"
      ],
      "metadata": {
        "id": "BCmuBJ9rGwb-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYfSAEIKRcsQ",
        "outputId": "96b437fd-d3e0-412d-f2d6-75d8453f7199"
      },
      "source": [
        "data = [\n",
        "        (1,'start','2021-01-01',100,'A'),\n",
        "        (1,'end','2021-01-31',200,'A'),\n",
        "        (2,'start','2021-03-05 4:53:11',100,'A'),\n",
        "        (2,'end','2021-05-01 05:06:38',200,'A'),\n",
        "]\n",
        "\n",
        "columns = [\"id\",\"session\",\"datetime\",\"station_return\",\"type\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.createOrReplaceTempView(\"window_test\")\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-------------------+--------------+----+\n",
            "| id|session|           datetime|station_return|type|\n",
            "+---+-------+-------------------+--------------+----+\n",
            "|  1|  start|         2021-01-01|           100|   A|\n",
            "|  1|    end|         2021-01-31|           200|   A|\n",
            "|  2|  start| 2021-03-05 4:53:11|           100|   A|\n",
            "|  2|    end|2021-05-01 05:06:38|           200|   A|\n",
            "+---+-------+-------------------+--------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZWxv4Z4WXsE"
      },
      "source": [
        "spark.sql('''\n",
        "SELECT\n",
        "  id,\n",
        "  datetime,\n",
        "  lead(datetime) OVER (PARTITION BY id ORDER BY datetime) as next_datetime,\n",
        "  DATEDIFF(lead(datetime) OVER (PARTITION BY id ORDER BY datetime),datetime) as duration_in_days\n",
        "\n",
        "FROM window_test\n",
        "\n",
        "''').show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
